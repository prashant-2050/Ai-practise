# The Transformer Model: Mathematical Deep Dive

## Mathematical Foundations and Implementation Details

This document extends the basic transformer explanation with comprehensive mathematical formulations and their implementations in our code.

---

## 1. Mathematical Notation and Core Concepts

### üìê Basic Mathematical Symbols Used

| Symbol | Meaning | In Our Code |
|--------|---------|-------------|
| **d_model** | Model dimension (embedding size) | `config.n_embd = 384` |
| **d_k, d_v** | Key/Value dimensions | `C // self.n_head = 64` |
| **n_h** | Number of attention heads | `config.n_head = 6` |
| **L** | Number of layers | `config.n_layer = 6` |
| **T** | Sequence length | `T` in our code |
| **V** | Vocabulary size | `config.vocab_size = 50257` |
| **‚äï** | Element-wise addition | `+` operator |
| **‚äô** | Element-wise multiplication | `*` operator |
| **||** | Concatenation | `torch.cat()` |
| **œÉ** | Activation function | `F.softmax()`, `F.gelu()` |

---

## 2. Input Representations: Mathematical Formulation

### üî¢ Token Embedding Matrix

**Mathematical Definition**:
```
E ‚àà ‚Ñù^(V √ó d_model)
where V = vocabulary size, d_model = embedding dimension
```

**In Our Implementation**:
```python
# light_llm.py
self.transformer.wte = nn.Embedding(config.vocab_size, config.n_embd)
# Creates matrix: E ‚àà ‚Ñù^(50257 √ó 384)
```

**Token Lookup Operation**:
```
x_i = E[token_id_i]
where x_i ‚àà ‚Ñù^d_model
```

**Code Example**:
```python
# For token_id = 464 ("The")
tok_emb = self.transformer.wte(idx)  # idx[0] = 464
# Result: tok_emb[0] = E[464] ‚àà ‚Ñù^384
```

### üìç Positional Encoding Matrix

**Mathematical Definition**:
```
P ‚àà ‚Ñù^(T_max √ó d_model)
where T_max = maximum sequence length
```

**Position Embedding Lookup**:
```
p_i = P[i] for position i
```

**In Our Code**:
```python
self.transformer.wpe = nn.Embedding(config.n_positions, config.n_embd)
# Creates: P ‚àà ‚Ñù^(1024 √ó 384)

pos = torch.arange(0, t, dtype=torch.long, device=device)
pos_emb = self.transformer.wpe(pos)  # p_i = P[i]
```

### üîÑ Input Combination

**Mathematical Formula**:
```
X = E_tokens + P_positions
where X ‚àà ‚Ñù^(T √ó d_model)
```

**Implementation**:
```python
x = self.transformer.drop(tok_emb + pos_emb)
# X[i] = E[token_i] + P[i] ‚àà ‚Ñù^384
```

---

## 3. Self-Attention: Complete Mathematical Framework

### üßÆ Query, Key, Value Projections

**Mathematical Formulation**:
```
Q = XW^Q,  where W^Q ‚àà ‚Ñù^(d_model √ó d_k)
K = XW^K,  where W^K ‚àà ‚Ñù^(d_model √ó d_k)  
V = XW^V,  where W^V ‚àà ‚Ñù^(d_model √ó d_v)

Typically: d_k = d_v = d_model / n_h
```

**In Our Implementation**:
```python
# Single linear layer produces all three matrices
self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)
# W ‚àà ‚Ñù^(384 √ó 1152), splits into 3 √ó ‚Ñù^(384 √ó 384)

qkv = self.c_attn(x)  # ‚Ñù^(B√óT√ó384) ‚Üí ‚Ñù^(B√óT√ó1152)
q, k, v = qkv.split(self.n_embd, dim=2)  # Each: ‚Ñù^(B√óT√ó384)
```

### üîÄ Multi-Head Reshaping

**Mathematical Transformation**:
```
Q_h = reshape(Q, [B, T, n_h, d_k]) 
K_h = reshape(K, [B, T, n_h, d_k])
V_h = reshape(V, [B, T, n_h, d_v])

Then transpose to: [B, n_h, T, d_k]
```

**Code Implementation**:
```python
# Reshape: [B, T, 384] ‚Üí [B, T, 6, 64] ‚Üí [B, 6, T, 64]
q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)
k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)
v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)
```

### ‚ö° Attention Score Computation

**Mathematical Formula**:
```
A = softmax(QK^T / ‚àöd_k) ‚àà ‚Ñù^(T √ó T)

Where:
- QK^T creates compatibility scores
- ‚àöd_k provides scale normalization
- softmax converts to probabilities
```

**Detailed Breakdown**:
```python
# Step 1: Compute raw scores
att = q @ k.transpose(-2, -1)  # [B, n_h, T, T]
# att[i,j] = Œ£(Q[i] ‚äô K[j]) = compatibility between positions i,j

# Step 2: Scale normalization  
att = att * (1.0 / math.sqrt(k.size(-1)))  # Divide by ‚àöd_k = ‚àö64 = 8
# Prevents softmax saturation for large d_k
```

**Why ‚àöd_k Scaling?**
- **Problem**: For large d_k, dot products grow large ‚Üí softmax saturates ‚Üí gradients vanish
- **Solution**: Scale by ‚àöd_k keeps dot products in reasonable range
- **Mathematical Intuition**: If Q,K ~ N(0,1), then QK^T ~ N(0, d_k), so QK^T/‚àöd_k ~ N(0,1)

### üé≠ Causal Masking

**Mathematical Definition**:
```
M ‚àà ‚Ñù^(T √ó T) where M[i,j] = {
    0 if i < j (future positions)
    1 if i ‚â• j (current and past positions)
}

A_masked = A ‚äô M + (1-M) √ó (-‚àû)
```

**Implementation**:
```python
# Create lower triangular mask
self.register_buffer("bias", torch.tril(torch.ones(n_positions, n_positions)))

# Apply mask (set future positions to -inf)
att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))
```

**Mask Visualization**:
```
For sequence "The cat sat":
       The  cat  sat
The  [  1,   0,   0 ]  ‚Üê "The" can only see itself
cat  [  1,   1,   0 ]  ‚Üê "cat" can see "The" and itself  
sat  [  1,   1,   1 ]  ‚Üê "sat" can see all previous tokens
```

### üéØ Attention Application

**Mathematical Formula**:
```
Output = Attention(Q,K,V) = softmax(QK^T / ‚àöd_k)V
```

**Step-by-Step**:
```python
# Step 1: Softmax normalization
att = F.softmax(att, dim=-1)  # Each row sums to 1
# att[i] = [p_i1, p_i2, ..., p_iT] where Œ£p_ij = 1

# Step 2: Weighted combination of values
y = att @ v  # [B, n_h, T, d_v]
# y[i] = Œ£(att[i,j] * V[j]) = weighted average of all values
```

### üîß Multi-Head Concatenation

**Mathematical Operation**:
```
MultiHead(Q,K,V) = Concat(head_1, head_2, ..., head_h)W^O
where head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)
```

**Implementation**:
```python
# Concatenate heads: [B, n_h, T, d_v] ‚Üí [B, T, n_h √ó d_v] = [B, T, d_model]
y = y.transpose(1, 2).contiguous().view(B, T, C)

# Output projection
y = self.resid_dropout(self.c_proj(y))  # W^O ‚àà ‚Ñù^(d_model √ó d_model)
```

---

## 4. Feed Forward Network: Mathematical Analysis

### üß† MLP Architecture

**Mathematical Definition**:
```
FFN(x) = max(0, xW_1 + b_1)W_2 + b_2
or in our case:
FFN(x) = GELU(xW_1 + b_1)W_2 + b_2

Where:
W_1 ‚àà ‚Ñù^(d_model √ó d_ff), typically d_ff = 4 √ó d_model
W_2 ‚àà ‚Ñù^(d_ff √ó d_model)
```

**In Our Implementation**:
```python
class MLP(nn.Module):
    def __init__(self, config):
        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)  # W_1: 384‚Üí1536
        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)  # W_2: 1536‚Üí384

    def forward(self, x):
        x = self.c_fc(x)        # x ‚àà ‚Ñù^(B√óT√ó384) ‚Üí ‚Ñù^(B√óT√ó1536)
        x = F.gelu(x)           # Apply GELU activation
        x = self.c_proj(x)      # ‚Ñù^(B√óT√ó1536) ‚Üí ‚Ñù^(B√óT√ó384)
        return x
```

### üìä GELU Activation Function

**Mathematical Definition**:
```
GELU(x) = x ¬∑ Œ¶(x) = x ¬∑ P(X ‚â§ x), where X ~ N(0,1)

Approximation: GELU(x) ‚âà 0.5x(1 + tanh(‚àö(2/œÄ)(x + 0.044715x¬≥)))
```

**Why GELU over ReLU?**
- **Smooth**: Differentiable everywhere (better gradients)
- **Probabilistic**: Based on normal distribution
- **Non-monotonic**: Can suppress less important features

**Implementation Detail**:
```python
x = F.gelu(x)  # PyTorch provides optimized GELU implementation
```

---

## 5. Layer Normalization: Mathematical Foundation

### üìè LayerNorm Formula

**Mathematical Definition**:
```
LayerNorm(x) = Œ≥ ‚äô (x - Œº)/œÉ + Œ≤

Where for each sample in batch:
Œº = (1/d_model) Œ£ x_i        (mean across features)
œÉ = ‚àö((1/d_model) Œ£ (x_i - Œº)¬≤)  (standard deviation)
Œ≥, Œ≤ ‚àà ‚Ñù^d_model are learned parameters
```

**Implementation**:
```python
self.ln_1 = nn.LayerNorm(config.n_embd)  # Œ≥, Œ≤ ‚àà ‚Ñù^384

# In forward pass:
# For each position in sequence, normalize across the 384 dimensions
normalized = self.ln_1(x)  # x ‚àà ‚Ñù^(B√óT√ó384) ‚Üí normalized ‚àà ‚Ñù^(B√óT√ó384)
```

**Why LayerNorm?**
- **Stability**: Keeps activations in reasonable range
- **Speed**: Reduces internal covariate shift
- **Independence**: Each sample normalized independently

---

## 6. Residual Connections: Mathematical Analysis

### üîÑ Residual Formula

**Mathematical Definition**:
```
y = x + F(x)
where F(x) is the transformation (attention or MLP)
```

**In Our Transformer Block**:
```python
def forward(self, x):
    # First residual connection
    x = x + self.attn(self.ln_1(x))    # x_new = x_old + Attention(LN(x_old))
    
    # Second residual connection  
    x = x + self.mlp(self.ln_2(x))     # x_final = x_new + MLP(LN(x_new))
    return x
```

**Mathematical Benefits**:
1. **Gradient Flow**: ‚àáx = ‚àáy(1 + ‚àáF(x)) ‚â• ‚àáy (gradients can't vanish)
2. **Identity Mapping**: If F(x) = 0, then y = x (preserve input)
3. **Easier Optimization**: Model can learn incremental improvements

---

## 7. Training Mathematics: Loss and Optimization

### üìâ Cross-Entropy Loss

**Mathematical Definition**:
```
L = -Œ£ y_true[i] √ó log(y_pred[i])

For language modeling:
L = -(1/T) Œ£ log(P(x_{t+1} | x_1, ..., x_t))
```

**Implementation**:
```python
# Compute logits for all positions and vocabulary
logits = self.lm_head(x)  # [B, T, V] = [4, 1023, 50257]

# Cross-entropy between predictions and targets
loss = F.cross_entropy(
    logits.view(-1, logits.size(-1)),  # Flatten: [B√óT, V]
    targets.view(-1),                   # Flatten: [B√óT]
    ignore_index=-1
)
```

**Detailed Cross-Entropy Calculation**:
```python
# For each position t, we have:
# logits[t] ‚àà ‚Ñù^V (raw scores for each vocabulary token)
# target[t] ‚àà {0, 1, ..., V-1} (true next token)

# Convert to probabilities
probs = F.softmax(logits[t], dim=-1)  # Œ£ probs[i] = 1

# Cross-entropy for this position
loss_t = -log(probs[target[t]])
```

### üéØ AdamW Optimizer Mathematics

**Adam Update Rule**:
```
m_t = Œ≤‚ÇÅm_{t-1} + (1-Œ≤‚ÇÅ)g_t           (momentum)
v_t = Œ≤‚ÇÇv_{t-1} + (1-Œ≤‚ÇÇ)g_t¬≤          (second moment)
mÃÇ_t = m_t / (1-Œ≤‚ÇÅ^t)                  (bias correction)
vÃÇ_t = v_t / (1-Œ≤‚ÇÇ^t)                  (bias correction)
Œ∏_t = Œ∏_{t-1} - Œ±(mÃÇ_t/(‚àövÃÇ_t + Œµ) + ŒªŒ∏_{t-1})
```

**Where**:
- g_t = gradient at step t
- Œ≤‚ÇÅ = 0.9 (momentum decay)
- Œ≤‚ÇÇ = 0.95 (second moment decay)  
- Œ± = learning rate
- Œª = weight decay coefficient
- Œµ = 1e-8 (numerical stability)

**Implementation**:
```python
self.optimizer = torch.optim.AdamW(
    optim_groups, 
    lr=learning_rate,           # Œ±
    betas=(0.9, 0.95),         # Œ≤‚ÇÅ, Œ≤‚ÇÇ
    weight_decay=weight_decay   # Œª
)
```

### üìà Learning Rate Scheduling

**Cosine Annealing with Warmup**:
```
lr(t) = {
    lr_max √ó (t/t_warmup)                           if t ‚â§ t_warmup
    lr_max √ó 0.5(1 + cos(œÄ(t-t_warmup)/(t_max-t_warmup)))  if t > t_warmup
}
```

**Implementation**:
```python
def lr_lambda(step):
    if step < self.warmup_steps:
        return step / self.warmup_steps  # Linear warmup
    else:
        decay_ratio = (step - self.warmup_steps) / (self.max_steps - self.warmup_steps)
        coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # Cosine decay
        return coeff
```

---

## 8. Text Generation: Probabilistic Sampling

### üé≤ Temperature Sampling

**Mathematical Formula**:
```
P_temp(x_i) = exp(logits_i / œÑ) / Œ£ exp(logits_j / œÑ)

Where œÑ is temperature:
œÑ ‚Üí 0: Deterministic (argmax)
œÑ = 1: Standard softmax  
œÑ ‚Üí ‚àû: Uniform sampling
```

**Implementation**:
```python
logits = logits / temperature  # Scale by temperature
probs = F.softmax(logits, dim=-1)  # Convert to probabilities
next_token = torch.multinomial(probs, num_samples=1)  # Sample
```

### üîù Top-K Sampling

**Mathematical Definition**:
```
P_topk(x_i) = {
    P(x_i) / Z  if x_i ‚àà TopK(logits)
    0           otherwise
}
Where Z = Œ£ P(x_j) for x_j ‚àà TopK(logits)
```

**Implementation**:
```python
# Keep only top-k logits, set others to -infinity
v, _ = torch.topk(logits, min(top_k, logits.size(-1)))
logits[logits < v[:, [-1]]] = -float('Inf')
```

### üéØ Top-P (Nucleus) Sampling

**Mathematical Definition**:
```
TopP(p) = {x_i : Œ£ P(x_j) ‚â§ p for j ‚àà sorted_indices[0:i]}
```

**Algorithm**:
1. Sort probabilities in descending order
2. Find smallest set where cumulative probability ‚â• p
3. Sample only from this set

**Implementation**:
```python
sorted_logits, sorted_indices = torch.sort(logits, descending=True)
cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)

# Remove tokens with cumulative probability above threshold
sorted_indices_to_remove = cumulative_probs > top_p
# Keep at least the first token
sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
sorted_indices_to_remove[..., 0] = 0
```

---

## 9. Computational Complexity Analysis

### ‚è±Ô∏è Time Complexity

**Self-Attention**:
```
O(T¬≤ √ó d_model + T √ó d_model¬≤)
= O(T¬≤d + Td¬≤)

Where T = sequence length, d = model dimension
```

**Feed Forward**:
```
O(T √ó d_model √ó d_ff) = O(T √ó d¬≤) 
(since d_ff = 4 √ó d_model)
```

**Total per Layer**:
```
O(T¬≤d + Td¬≤)
```

**Full Model** (L layers):
```
O(L √ó (T¬≤d + Td¬≤))
```

### üíæ Memory Complexity

**Attention Matrices**:
```
O(n_h √ó T¬≤) for attention scores
= O(6 √ó 1024¬≤) ‚âà 6M parameters for our model
```

**Activations**:
```
O(L √ó T √ó d_model) for storing intermediate activations
= O(6 √ó 1024 √ó 384) ‚âà 2.4M parameters
```

**Gradients** (during training):
```
O(model_parameters) = O(30M) for backward pass storage
```

---

## 10. Numerical Stability Considerations

### üî¢ Gradient Clipping

**Mathematical Motivation**:
```
If ||‚àá|| > threshold:
    ‚àá_clipped = ‚àá √ó (threshold / ||‚àá||)
```

**Implementation**:
```python
torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip)
```

### üìä Weight Initialization

**Xavier/Glorot Initialization**:
```
W ~ N(0, 2/(fan_in + fan_out))
```

**Our Implementation**:
```python
def _init_weights(self, module):
    if isinstance(module, nn.Linear):
        torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
        if module.bias is not None:
            torch.nn.init.zeros_(module.bias)
```

### ‚öñÔ∏è Numerical Precision

**Mixed Precision Training** (ready for implementation):
```python
# Use torch.cuda.amp.autocast() for FP16 computation
with torch.cuda.amp.autocast():
    logits, loss = model(inputs, targets)
```

---

## Summary: Mathematical Architecture

Our transformer implements the complete mathematical framework:

**Forward Pass**:
```
X‚ÇÄ = TokenEmb(tokens) + PosEmb(positions)
For l = 1 to L:
    X_l = X_{l-1} + MultiHeadAttention(LayerNorm(X_{l-1}))
    X_l = X_l + FFN(LayerNorm(X_l))
Output = Softmax(X_L √ó W_vocab)
```

**Training**:
```
Loss = CrossEntropy(Output, Targets)
Gradients = ‚àá_Œ∏ Loss
Œ∏_new = AdamW(Œ∏_old, Gradients)
```

**Generation**:
```
For t = 1 to max_length:
    P(x_t | x_{<t}) = Transformer(x_{<t})
    x_t ~ Sample(P, temperature, top_k, top_p)
```

This mathematical foundation enables our 30M parameter model to learn language patterns and generate coherent text through the elegant interplay of linear algebra, probability theory, and optimization mathematics! üßÆ‚ú®