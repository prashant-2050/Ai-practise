name: Train LLM Model

on:
  workflow_dispatch:  # Manual trigger
    inputs:
      model_size:
        description: 'Model size to train'
        required: true
        default: 'nano'
        type: choice
        options:
          - nano
          - micro
          - small
      max_steps:
        description: 'Maximum training steps'
        required: true
        default: '100'
        type: string
      learning_rate:
        description: 'Learning rate'
        required: true
        default: '3e-4'
        type: string
  
  schedule:
    - cron: '0 2 * * 0'  # Weekly training on Sundays at 2 AM UTC

env:
  PYTHON_VERSION: '3.11'
  PYTORCH_VERSION: '2.0.0'

jobs:
  train:
    runs-on: ubuntu-latest
    timeout-minutes: 360  # 6 hours max
    env:
      MODEL_SIZE: ${{ github.event.inputs.model_size || 'nano' }}
      MAX_STEPS: ${{ github.event.inputs.max_steps || '100' }}
      LEARNING_RATE: ${{ github.event.inputs.learning_rate || '3e-4' }}
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y build-essential
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install tensorboard wandb  # Additional training tools
    
    - name: Download training data
      run: |
        if [ ! -f "shakespeare.txt" ]; then
          echo "Downloading Shakespeare dataset..."
          wget -O shakespeare.txt https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt
        fi
        echo "âœ… Training data ready"
    
    - name: Set training parameters
      run: |
        echo "MODEL_SIZE=${{ github.event.inputs.model_size || 'nano' }}" >> $GITHUB_ENV
        echo "MAX_STEPS=${{ github.event.inputs.max_steps || '100' }}" >> $GITHUB_ENV
        echo "LEARNING_RATE=${{ github.event.inputs.learning_rate || '3e-4' }}" >> $GITHUB_ENV
        echo "ðŸŽ¯ Training configuration:"
        echo "  Model size: ${{ github.event.inputs.model_size || 'nano' }}"
        echo "  Max steps: ${{ github.event.inputs.max_steps || '100' }}"
        echo "  Learning rate: ${{ github.event.inputs.learning_rate || '3e-4' }}"
    
    - name: Create checkpoints directory
      run: |
        mkdir -p checkpoints
        mkdir -p logs
        echo "ðŸ“ Directories created"
        
        # System diagnostics
        echo "ðŸ” System resources:"
        echo "  CPU cores: $(nproc)"
        echo "  Memory: $(free -h)"
        echo "  Disk space: $(df -h /)"
        echo "  Python processes: $(ps aux | grep python | wc -l)"
    
    - name: Train model
      run: |
        set -euxo pipefail
        echo "ðŸš€ Starting training..."
        echo "Memory before training: $(free -h)"
        
        # start a background heartbeat to keep alive logs in case of long silence
        (while true; do echo "[heartbeat] $(date) - Memory: $(free -m | grep '^Mem:' | awk '{print $3"MB used"}')"; sleep 60; done) &
        HEARTBEAT_PID=$!

        mkdir -p logs
        # Use timeout command to limit training time and add memory monitoring
        timeout 1800 python train_cloud.py \
          --model_size "$MODEL_SIZE" \
          --max_steps "$MAX_STEPS" \
          --learning_rate "$LEARNING_RATE" \
          --batch_size "${BATCH_SIZE:-16}" \
          --gradient_accumulation_steps "${GRAD_ACCUM_STEPS:-2}" \
          --device "${DEVICE:-cpu}" \
          --mixed_precision "${GPU_AVAILABLE:-false}" \
          --save_dir checkpoints \
          --log_dir logs \
          --eval_interval 25 \
          --save_interval 50 2>&1 | tee logs/training.log

        # stop heartbeat
        kill "$HEARTBEAT_PID" || true
        echo "Memory after training: $(free -h)"
    
    - name: Generate sample outputs
      run: |
        echo "ðŸŽ­ Generating sample outputs..."
        if [ -f "checkpoints/best_model.pt" ]; then
          python generate_cli.py \
            --checkpoint checkpoints/best_model.pt \
            --prompt "To be or not to be" \
            --max_tokens 100 \
            --temperature 0.8 \
            > sample_outputs.txt
        else
          echo "No best model found, using demo mode..."
          python generate.py demo > sample_outputs.txt
        fi
        
        echo "## ðŸ“ Sample Generated Text" >> training_summary.md
        echo '```' >> training_summary.md
        cat sample_outputs.txt >> training_summary.md
        echo '```' >> training_summary.md
    
    - name: Create training summary
      run: |
        echo "# ðŸŽ¯ Training Summary" > training_summary.md
        echo "" >> training_summary.md
        echo "**Model**: ${{ env.MODEL_SIZE }}" >> training_summary.md
        echo "**Steps**: ${{ env.MAX_STEPS }}" >> training_summary.md
        echo "**Learning Rate**: ${{ env.LEARNING_RATE }}" >> training_summary.md
        echo "**Date**: $(date)" >> training_summary.md
        echo "" >> training_summary.md
        
        if [ -f "logs/training.log" ]; then
          echo "## ðŸ“Š Training Logs" >> training_summary.md
          echo '```' >> training_summary.md
          tail -20 logs/training.log >> training_summary.md
          echo '```' >> training_summary.md
        fi
    
    - name: Upload model checkpoints
      uses: actions/upload-artifact@v4
      with:
        name: model-checkpoints-${{ env.MODEL_SIZE }}-${{ github.run_number }}
        path: |
          checkpoints/
          !checkpoints/checkpoint_step_*.pt
        retention-days: 30
    
    - name: Upload training logs
      uses: actions/upload-artifact@v4
      with:
        name: training-logs-${{ github.run_number }}
        path: |
          logs/
          training_summary.md
          sample_outputs.txt
        retention-days: 30
    
    - name: Upload to Hugging Face (if secrets available)
      if: env.HF_TOKEN != '' && env.HF_USERNAME != ''
      env:
        HF_TOKEN: ${{ secrets.HF_TOKEN }}
        HF_USERNAME: ${{ secrets.HF_USERNAME }}
      run: |
        pip install huggingface_hub
        python -c "
        import os
        from huggingface_hub import HfApi, login
        
        # Login to HF
        login(token=os.environ['HF_TOKEN'])
        api = HfApi()
        
        # Upload model
        try:
            api.upload_file(
                path_or_fileobj='checkpoints/best_model.pt',
                path_in_repo='pytorch_model.pt',
                repo_id=f\"{os.environ['HF_USERNAME']}/llm-{os.environ['MODEL_SIZE']}-model\",
                repo_type='model',
                create_repo=True
            )
            print('âœ… Model uploaded to Hugging Face')
        except Exception as e:
            print(f'âš ï¸ Could not upload to HF: {e}')
        "
    
    - name: Training Summary
      run: |
        echo "## ðŸŽ‰ Training Complete!" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "âœ… **Model**: ${{ env.MODEL_SIZE }} (${{ env.MAX_STEPS }} steps)" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "ðŸ“¦ **Artifacts Created**:" >> $GITHUB_STEP_SUMMARY
        echo "- Model checkpoints (best_model.pt, final_model.pt)" >> $GITHUB_STEP_SUMMARY
        echo "- Training logs and metrics" >> $GITHUB_STEP_SUMMARY
        echo "- Sample generated text" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "ðŸ”— **Download**: Check the 'Artifacts' section below" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        if [ -f "sample_outputs.txt" ]; then
          echo "ðŸŽ­ **Sample Output**:" >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
          head -5 sample_outputs.txt >> $GITHUB_STEP_SUMMARY
          echo '```' >> $GITHUB_STEP_SUMMARY
        fi